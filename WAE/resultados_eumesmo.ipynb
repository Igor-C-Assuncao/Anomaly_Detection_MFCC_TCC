{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95668b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAE-GAN Evaluation Notebook\n",
    "\n",
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from ArgumentParser import parse_arguments\n",
    "from train_cycles_adversarial import load_parameters, main, calculate_test_losses\n",
    "\n",
    "#  Helper: load resultados por fold e extrair ground-truth + scores\n",
    "def load_scores_and_labels(pkl_path):\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        result = pkl.load(f)\n",
    "    \n",
    "    y_true = np.array([0] * len(result['train']['reconstruction']) + [1] * len(result['test']['reconstruction']))\n",
    "    y_score = np.array(result['train']['reconstruction'] + result['test']['reconstruction'])\n",
    "    return y_true, y_score\n",
    "\n",
    "#  Helper: calcula intervalo de confiança bootstrap da ROC-AUC\n",
    "def bootstrap_roc_auc(y_true, y_score, n_bootstrap=1000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        score = roc_auc_score(y_true[indices], y_score[indices])\n",
    "        scores.append(score)\n",
    "    sorted_scores = np.sort(scores)\n",
    "    lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "    upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "    return np.mean(scores), lower, upper\n",
    "\n",
    "#  Helper: métricas padronizadas\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1\": f1_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "#  Avalia os 10 folds e agrega resultados\n",
    "def evaluate_all_folds(base_path, folds=10):\n",
    "    all_metrics = []\n",
    "    all_auc_scores = []\n",
    "\n",
    "    for i in range(folds):\n",
    "        pkl_path = os.path.join(base_path, f\"final_complete_losses_WAE_LSTMDiscriminator_analog_feats_8_2_10_0.001_0.001_fan_id_00_10_0.001_0.001_fold{i}.pkl\")\n",
    "        y_true, y_score = load_scores_and_labels(pkl_path)\n",
    "        y_pred = (y_score > np.percentile(y_score, 95)).astype(int)\n",
    "\n",
    "        metrics = compute_metrics(y_true, y_pred)\n",
    "        auc = roc_auc_score(y_true, y_score)\n",
    "        all_metrics.append(metrics)\n",
    "        all_auc_scores.append(auc)\n",
    "\n",
    "    mean_metrics = {k: np.mean([m[k] for m in all_metrics]) for k in all_metrics[0]}\n",
    "    std_metrics = {k: np.std([m[k] for m in all_metrics]) for k in all_metrics[0]}\n",
    "    auc_mean, auc_low, auc_high = bootstrap_roc_auc(np.array(y_true), np.array(y_score))\n",
    "\n",
    "    return mean_metrics, std_metrics, auc_mean, auc_low, auc_high\n",
    "\n",
    "# ▶ Executar avaliação\n",
    "base_result_path = \"results/\"\n",
    "mean_metrics, std_metrics, auc_mean, auc_low, auc_high = evaluate_all_folds(base_result_path)\n",
    "\n",
    "print(\"\\n\\u2728 Resultados agregados (10-fold cross-validation):\")\n",
    "for k in mean_metrics:\n",
    "    print(f\"{k.capitalize()}: {mean_metrics[k]:.3f} ± {std_metrics[k]:.3f}\")\n",
    "print(f\"ROC-AUC: {auc_mean:.3f} (95% CI: {auc_low:.3f} - {auc_high:.3f})\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
