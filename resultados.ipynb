{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6fd777a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "10961524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e281c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import argparse # Para construir o objeto args\n",
    "from tqdm.notebook import tqdm\n",
    "import librosa\n",
    "import torch\n",
    "import sys # Para adicionar caminhos\n",
    "import shutil # Para copiar/mover arquivos temporários, se necessário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "00ec48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOGLE COLAB\n",
    "\n",
    "try:\n",
    "    # Tenta detectar se está no Colab para definir um caminho base diferente\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    BASE_PROJECT_PATH = \"/content/Anomaly_Detection_MFCC_TCC/\" # Exemplo se clonado no Colab\n",
    "    # Monte seu Google Drive se os dados/código estiverem lá\n",
    "    # from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "    # BASE_PROJECT_PATH = \"/content/drive/MyDrive/TCC/Anomaly_Detection_MFCC_TCC/\"\n",
    "    # !git clone https://github.com/Igor-C-Assuncao/Anomaly_Detection_MFCC_TCC.git $BASE_PROJECT_PATH \n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    BASE_PROJECT_PATH = \"C:\\\\Users\\\\igorc\\\\Desktop\\\\Implementação TCC\"  # SEU CAMINHO LOCAL AQUI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0d262fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar caminhos dos seus módulos ao sys.path para importação\n",
    "sys.path.append(os.path.abspath(os.path.join(BASE_PROJECT_PATH))) \n",
    "sys.path.append(os.path.abspath(os.path.join(BASE_PROJECT_PATH, \"mtsa\")))\n",
    "sys.path.append(os.path.abspath(os.path.join(BASE_PROJECT_PATH, \"WAE\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2f61d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Importações dos seus Módulos (APÓS ADICIONAR AO SYS.PATH) -----\n",
    "from mtsa.utils import files_train_test_split, Wav2Array\n",
    "from mtsa.features.mel import Array2Mfcc\n",
    "\n",
    "from WAE import ArgumentParser # Assume que ArgumentParser.py está em WAE/ e define parse_arguments()\n",
    "from WAE.models.TCN_AAE import Encoder_TCN, Decoder_TCN, LSTMDiscriminator_TCN\n",
    "from WAE.models.LSTM_AAE import Encoder as LSTMEncoder, Decoder as LSTMDecoder, SimpleDiscriminator # etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca156cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMII_DATA_PATH = os.path.join(BASE_PROJECT_PATH, \"Data\", \"MIMII\")\n",
    "SCRIPTS_WAE_PATH = os.path.join(BASE_PROJECT_PATH, \"WAE\")\n",
    "GLOBAL_PREPROCESSED_DATA_PATH = os.path.join(BASE_PROJECT_PATH, \"Data\", \"global_preprocessed_notebook\")\n",
    "FOLD_SPECIFIC_TEMP_PATH = os.path.join(BASE_PROJECT_PATH, \"evaluation_temp_notebook\")\n",
    "FINAL_RESULTS_PATH = os.path.join(BASE_PROJECT_PATH, \"evaluation_results_notebook\")\n",
    "\n",
    "os.makedirs(GLOBAL_PREPROCESSED_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(FOLD_SPECIFIC_TEMP_PATH, exist_ok=True)\n",
    "os.makedirs(FINAL_RESULTS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d651c534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# ----- evaluation config -----\n",
    "N_FOLDS = 10 \n",
    "N_BOOTSTRAP_SAMPLES = 1000 #  IC  ROC-AUC\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b1ce360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ARGS_DICT = {\n",
    "    \"lr\": 1e-3, \"disc_lr\": 1e-3, \"epochs\": 50, \"weight_decay\": 0,\n",
    "    \"critic_iterations\": 5, \"GP_hyperparam\": 10.0, \"WAE_regularization_term\": 10.0,\n",
    "    \"dropout\": 0.1, \"embedding\": 64, \"hidden_dims\": 30, \"lstm_layers\": 2,\n",
    "    \"batch_size\": 32, \"disc_hidden\": 32, \"disc_layers\": 3,\n",
    "    \"tcn_hidden\": 30, \"tcn_layers\": 10, \"tcn_kernel\": 3,\n",
    "    \"sparsity_weight\": 1.0, \"sparsity_parameter\": 0.05, \"nheads\": 8,\n",
    "    \"feats\": \"all\", # Será usado para construir nome de arquivo, mas number_features virá dos dados\n",
    "    \"NUMBER_FEATURES\": 20, # Placeholder, será atualizado   \n",
    "    \"successive_iters\": 10, \"delta_worse\": 0.02, \"delta_better\": 0.001,\n",
    "    \"model_name\": \"LSTMDiscriminator_TCN\", # Ou o que for seu WAE-GAN principal\n",
    "    \"encoder_name\": \"TCN\", \"decoder_name\": \"TCN\",\n",
    "    \"reconstruction_error_metric\": \"mse\", \"dtw_local_size\": 5,\n",
    "    \"separate_comp\": False, \"init_loop\": 0, \"end_loop\": 17, # Estes podem não ser relevantes para CV\n",
    "    \"force_training\": True, # Para garantir que treine em cada fold\n",
    "    \"sensor\": \"tp2\",\n",
    "    \"use_discriminator\": True, # Para WAE-GAN\n",
    "    \"machine_type\": \"\", \"machine_id\": \"\", # Preenchidos no loop\n",
    "    # Adicionar os novos argumentos de override\n",
    "    \"input_train_data_path\": None, \"input_test_data_path\": None,\n",
    "    \"output_model_base_path\": None, \"output_results_pkl_path\": None,\n",
    "    # Adicione quaisquer outros argumentos que seu ArgumentParser define com defaults\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b7baaf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MACHINE_CONFIGS = {\n",
    "#     \"fan\": [\"id_00\", \"id_02\", \"id_04\", \"id_06\"],\n",
    "#     \"pump\": [\"id_00\", \"id_02\", \"id_04\", \"id_06\"],\n",
    "#     \"slider\": [\"id_00\", \"id_02\", \"id_04\", \"id_06\"],\n",
    "#     \"valve\": [\"id_00\", \"id_02\", \"id_04\", \"id_06\"],\n",
    "# }\n",
    "\n",
    "MACHINE_CONFIGS = {\n",
    "    \"fan\": [\"id_00\" ],\n",
    "    \"pump\": [\"id_00\"],\n",
    "    \"slider\": [\"id_00\"],\n",
    "    \"valve\": [\"id_00\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cec788aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2: Função de Carga e Pré-processamento Global por Máquina\n",
    "# (Usando files_train_test_split para obter caminhos e Array2Mfcc implicitamente via librosa)\n",
    "\n",
    "# (Importações da Célula 1 já devem incluir Wav2Array, Array2Mfcc, e files_train_test_split)\n",
    "import librosa # Para chamada direta a librosa.feature.mfcc\n",
    "\n",
    "def get_all_mfcc_data_for_machine(machine_type, machine_id, num_mfcc_coeffs=20, target_sr=16000, force_reprocess=False):\n",
    "    \"\"\"\n",
    "    Carrega ou pré-processa todos os dados MFCC para uma máquina/ID.\n",
    "    Usa files_train_test_split para obter a lista de todos os caminhos de arquivo e seus rótulos.\n",
    "    Salva/carrega de GLOBAL_PREPROCESSED_DATA_PATH para evitar reprocessamento.\n",
    "    Retorna:\n",
    "        X_all_tensors (list): Lista de tensores PyTorch (1, n_frames, n_coeffs).\n",
    "        y_all_labels_original (np.array): Rótulos (Normal=1, Anormal=0, conforme mtsa.utils).\n",
    "        actual_n_coeffs (int): Número real de coeficientes MFCC.\n",
    "    \"\"\"\n",
    "    global_pkl_file_name = f\"{machine_type}_{machine_id}_all_mfcc_data_c{num_mfcc_coeffs}.pkl\"\n",
    "    global_pkl_path = os.path.join(GLOBAL_PREPROCESSED_DATA_PATH, global_pkl_file_name)\n",
    "\n",
    "    if not force_reprocess and os.path.exists(global_pkl_path):\n",
    "        print(f\"  Carregando dados MFCC pré-processados de: {global_pkl_path}\")\n",
    "        with open(global_pkl_path, \"rb\") as f:\n",
    "            data = pkl.load(f)\n",
    "        if data.get(\"num_mfcc_coeffs_requested\") == num_mfcc_coeffs: # Checa se foi processado com os mesmos params\n",
    "            return data[\"X_all_tensors\"], data[\"y_all_labels_original\"], data[\"actual_n_coeffs\"]\n",
    "        else:\n",
    "            print(f\"    Parâmetros de MFCC no arquivo ({data.get('num_mfcc_coeffs_requested')}) diferem do solicitado ({num_mfcc_coeffs}). Reprocessando.\")\n",
    "\n",
    "    print(f\"  Processando arquivos .wav para {machine_type}/{machine_id} com {num_mfcc_coeffs} coeficientes MFCC...\")\n",
    "    id_path = os.path.join(MIMII_DATA_PATH, machine_type, machine_id)\n",
    "    if not os.path.isdir(id_path):\n",
    "        print(f\"    Diretório não encontrado: {id_path}\")\n",
    "        return [], np.array([]), 0\n",
    "\n",
    "    # files_train_test_split retorna X_train_files, X_test_files, y_train_labels, y_test_labels\n",
    "    # Os rótulos são Normal=1, Anormal=0.\n",
    "    # Para a CV, queremos todos os dados juntos primeiro.\n",
    "    X_train_files, X_test_files, y_train_original, y_test_original = files_train_test_split(id_path, random_state=42) # random_state para consistência na divisão inicial se usada\n",
    "\n",
    "    all_wav_files = np.concatenate((X_train_files, X_test_files))\n",
    "    all_original_labels = np.concatenate((y_train_original, y_test_original))\n",
    "\n",
    "    if len(all_wav_files) == 0:\n",
    "        print(f\"    Nenhum arquivo .wav encontrado ou combinado para {id_path}.\")\n",
    "        return [], np.array([]), 0\n",
    "\n",
    "    wav_to_array_transformer = Wav2Array(sampling_rate=target_sr, mono=True)\n",
    "    # Se você for usar sua classe Array2Mfcc, e ela não aceita n_mfcc,\n",
    "    # ela usará o default do librosa (provavelmente 20).\n",
    "    # Para ter controle, a chamada direta ao librosa.feature.mfcc é mais explícita.\n",
    "    # array_to_mfcc_transformer = Array2Mfcc(sampling_rate=target_sr) # Se usar este, n_mfcc pode ser fixo.\n",
    "\n",
    "    X_all_tensors = []\n",
    "    valid_indices_for_labels = []\n",
    "    actual_n_coeffs_detected = 0\n",
    "\n",
    "    print(f\"    Convertendo {len(all_wav_files)} arquivos .wav para arrays...\")\n",
    "    # É mais robusto processar WAVs um por um se alguns puderem estar corrompidos\n",
    "    audio_arrays_list = []\n",
    "    valid_original_indices_wav = []\n",
    "    for idx, wav_file_path in enumerate(tqdm(all_wav_files, desc=\"      Processando WAVs\")):\n",
    "        try:\n",
    "            # Wav2Array.transform espera uma lista de caminhos\n",
    "            audio_array_single = wav_to_array_transformer.transform([wav_file_path])[0]\n",
    "            if audio_array_single.ndim > 0 and audio_array_single.size > 0:\n",
    "                audio_arrays_list.append(audio_array_single)\n",
    "                valid_original_indices_wav.append(idx) # Guarda o índice do arquivo original que foi bem sucedido\n",
    "        except Exception as e_single_wav:\n",
    "            print(f\"        Erro no arquivo {wav_file_path}: {e_single_wav}. Pulando arquivo.\")\n",
    "    \n",
    "    if not audio_arrays_list:\n",
    "         print(f\"    Nenhum arquivo .wav pôde ser convertido para array.\")\n",
    "         return [], np.array([]), 0\n",
    "    \n",
    "    # Filtra os rótulos para corresponder apenas aos WAVs processados com sucesso\n",
    "    all_original_labels_filtered = all_original_labels[valid_original_indices_wav]\n",
    "\n",
    "    print(f\"    Extraindo e normalizando MFCCs de {len(audio_arrays_list)} arrays de áudio...\")\n",
    "    for i, audio_array in enumerate(tqdm(audio_arrays_list, desc=\"      Processando MFCCs\")):\n",
    "        try:\n",
    "            # Chamada direta ao librosa.feature.mfcc para controle de n_mfcc\n",
    "            mfcc_cycle_raw = librosa.feature.mfcc(y=audio_array, sr=target_sr, n_mfcc=num_mfcc_coeffs)\n",
    "            # mfcc_cycle_raw shape: (n_mfcc, n_frames)\n",
    "\n",
    "            if mfcc_cycle_raw.shape[1] == 0: continue\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            mfcc_transposed_for_scaler = mfcc_cycle_raw.T # (n_frames, n_mfcc_coeffs)\n",
    "            \n",
    "            if mfcc_transposed_for_scaler.shape[0] == 0: continue\n",
    "            if mfcc_transposed_for_scaler.ndim == 1:\n",
    "                 mfcc_transposed_for_scaler = mfcc_transposed_for_scaler.reshape(1, -1)\n",
    "\n",
    "            scaled_mfcc_transposed = scaler.fit_transform(mfcc_transposed_for_scaler)\n",
    "            \n",
    "            tensor_cycle = torch.tensor(scaled_mfcc_transposed, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "            # tensor_cycle shape: (1, n_frames, n_mfcc_coeffs)\n",
    "            \n",
    "            X_all_tensors.append(tensor_cycle)\n",
    "            valid_indices_for_labels.append(i) # Índice relativo à audio_arrays_list\n",
    "\n",
    "            if actual_n_coeffs_detected == 0 and tensor_cycle.shape[2] > 0:\n",
    "                actual_n_coeffs_detected = tensor_cycle.shape[2]\n",
    "                if actual_n_coeffs_detected != num_mfcc_coeffs:\n",
    "                    print(f\"    ALERTA: num_mfcc_coeffs solicitado={num_mfcc_coeffs}, detectado={actual_n_coeffs_detected}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      Erro processando MFCC para um arquivo (índice {i} da lista de áudio): {e}. Pulando.\")\n",
    "            continue\n",
    "            \n",
    "    if not X_all_tensors:\n",
    "        print(f\"    Nenhum ciclo MFCC válido gerado para {machine_type}/{machine_id}.\")\n",
    "        return [], np.array([]), 0\n",
    "    \n",
    "    # Filtra os rótulos finais para corresponder apenas aos MFCCs que foram processados com sucesso\n",
    "    y_all_labels_processed = all_original_labels_filtered[valid_indices_for_labels]\n",
    "\n",
    "    data_to_save = {\n",
    "        \"X_all_tensors\": [t.cpu() for t in X_all_tensors],\n",
    "        \"y_all_labels_original\": y_all_labels_processed,\n",
    "        \"actual_n_coeffs\": actual_n_coeffs_detected,\n",
    "        \"num_mfcc_coeffs_requested\": num_mfcc_coeffs\n",
    "    }\n",
    "    with open(global_pkl_path, \"wb\") as f:\n",
    "        pkl.dump(data_to_save, f)\n",
    "    print(f\"  Dados MFCC ({len(X_all_tensors)} ciclos) processados e salvos em: {global_pkl_path}\")\n",
    "\n",
    "    return X_all_tensors, y_all_labels_processed, actual_n_coeffs_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5bfc0667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3: Funções para Chamar Scripts .py e Manipular Dados de Fold\n",
    "\n",
    "def save_fold_data_for_script(tensors_list, save_path):\n",
    "    \"\"\"Salva uma lista de tensores PyTorch em um arquivo .pkl.\"\"\"\n",
    "    # Os scripts esperam uma lista de tensores, onde cada tensor pode ser (1, seq_len, n_features)\n",
    "    # Seus scripts `train_cycles_*.py` carregam com pkl.load() e depois convertem para device.\n",
    "    # Esta função apenas salva a lista de tensores (que já estão no DEVICE correto se pré-processados no notebook).\n",
    "    # No entanto, para manter a consistência com o que os scripts podem esperar de arquivos pkl\n",
    "    # gerados pelo preprocessing_mimii.py, pode ser mais seguro salvar listas de arrays numpy\n",
    "    # e deixar os scripts converterem para tensor e moverem para o device.\n",
    "    # Mas se os scripts já lidam com tensores do device correto, está ok.\n",
    "\n",
    "    # Vamos assumir que salvamos a lista de tensores diretamente.\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pkl.dump(tensors_list, f)\n",
    "\n",
    "def run_script_for_fold(script_executable_path, args_for_script_dict):\n",
    "    \"\"\"\n",
    "    Executa um script Python (e.g., train_cycles_adversarial.py) como um subprocesso.\n",
    "    args_for_script_dict: dicionário de argumentos para o script.\n",
    "    Retorna True se sucesso, False caso contrário.\n",
    "    \"\"\"\n",
    "    cmd = [sys.executable, script_executable_path]\n",
    "    for arg_name, arg_value in args_for_script_dict.items():\n",
    "        if isinstance(arg_value, bool):\n",
    "            if arg_value:\n",
    "                cmd.append(f\"--{arg_name}\")  # Use underscore\n",
    "        elif arg_value is not None:\n",
    "            cmd.append(f\"--{arg_name}\")  # Use underscore\n",
    "            cmd.append(str(arg_value))\n",
    "\n",
    "    print(f\"    Executando: {' '.join(cmd)}\")\n",
    "    try:\n",
    "        # Timeout aumentado para 60 minutos (3600s) para treinos mais longos\n",
    "        process = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=3600, encoding='utf-8', errors='replace')\n",
    "        # print(f\"      Output do Script (stdout):\\n{process.stdout[-500:]}\") # Últimas linhas\n",
    "        # if process.stderr:\n",
    "        #     print(f\"      Output do Script (stderr):\\n{process.stderr[-500:]}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"    ERRO ao executar script: {script_executable_path}\")\n",
    "        print(f\"      Comando: {' '.join(e.cmd)}\")\n",
    "        print(f\"      Código de Saída: {e.returncode}\")\n",
    "        print(f\"      Stderr: {e.stderr}\")\n",
    "        print(f\"      Stdout: {e.stdout}\")\n",
    "        return False\n",
    "    except subprocess.TimeoutExpired as e:\n",
    "        print(f\"    TIMEOUT ({e.timeout}s) ao executar script: {script_executable_path}\")\n",
    "        print(f\"      Comando: {' '.join(e.cmd)}\")\n",
    "        print(f\"      Stderr (parcial): {e.stderr}\")\n",
    "        print(f\"      Stdout (parcial): {e.stdout}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"    Exceção inesperada ao executar script {script_executable_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_scores_from_script_output(expected_pkl_path):\n",
    "    \"\"\"\n",
    "    Carrega os scores de anomalia do arquivo .pkl salvo pelo script de treino/predição.\n",
    "    Espera que o .pkl contenha um dicionário com a chave \"test\" e dentro dela \"reconstruction\" e/ou \"critic\".\n",
    "    \"\"\"\n",
    "    if not os.path.exists(expected_pkl_path):\n",
    "        print(f\"    Arquivo de scores não encontrado: {expected_pkl_path}\")\n",
    "        return None, None # reconstruction_errors, critic_scores\n",
    "\n",
    "    try:\n",
    "        with open(expected_pkl_path, \"rb\") as f:\n",
    "            results_data = pkl.load(f)\n",
    "\n",
    "        # Estrutura esperada baseada em train_cycles_adversarial.py:\n",
    "        # results = {\"test\": {\"reconstruction\": reconstruction_errors, \"critic\": critic_scores},\n",
    "        #            \"train\": {\"reconstruction\": args.train_reconstruction_errors, \"critic\": args.train_critic_scores}}\n",
    "        # Para train_cycles.py (AE simples):\n",
    "        # losses_over_time = {\"test\": test_losses, \"train\": args.train_losses} (test_losses são os erros de reconstrução)\n",
    "\n",
    "        if \"test\" not in results_data:\n",
    "            print(f\"    Chave 'test' não encontrada no arquivo de scores: {expected_pkl_path}\")\n",
    "            return None, None\n",
    "\n",
    "        test_results = results_data[\"test\"]\n",
    "        reconstruction_errors = np.array(test_results.get(\"reconstruction\", []))\n",
    "        critic_scores = np.array(test_results.get(\"critic\", [])) # Pode ser vazio se AE simples\n",
    "\n",
    "        # Se for AE simples, \"test_losses\" podem estar diretamente em results_data[\"test\"]\n",
    "        if not reconstruction_errors.any() and isinstance(test_results, list): # Caso do train_cycles.py\n",
    "            reconstruction_errors = np.array(test_results)\n",
    "            critic_scores = np.array([]) # Sem scores do crítico para AE simples\n",
    "\n",
    "        if not reconstruction_errors.any():\n",
    "             print(f\"    Nenhum score de reconstrução encontrado em 'test' no arquivo: {expected_pkl_path}\")\n",
    "             return None, None\n",
    "\n",
    "        return reconstruction_errors, critic_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    Erro ao carregar ou processar arquivo de scores {expected_pkl_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Função para determinar limiar ótimo (da resposta anterior, pode ir aqui)\n",
    "def find_optimal_threshold_roc(y_true_binary_anomaly, anomaly_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true_binary_anomaly, anomaly_scores)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    if optimal_threshold == np.inf or optimal_threshold == -np.inf or np.isnan(optimal_threshold):\n",
    "        # print(f\"    Aviso: Limiar ótimo da ROC foi {optimal_threshold}. Usando mediana.\")\n",
    "        return np.median(anomaly_scores)\n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c7a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processando Máquina: fan_id_00 =====\n",
      "  Carregando dados MFCC pré-processados de: C:\\Users\\igorc\\Desktop\\Implementação TCC\\Data\\global_preprocessed_notebook\\fan_id_00_all_mfcc_data_c20.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147c10ef13524c788f9ef3559789a7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Folds fan_id_00:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  --- Fold 1/10 para fan_id_00 ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'EPOCHS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m args_for_this_fold[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_model_base_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(fold_temp_dir_current, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_fold\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m args_for_this_fold[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_results_pkl_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(fold_temp_dir_current, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores_fold.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m args_for_this_fold[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCHS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m DEFAULT_ARGS_DICT[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCHS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# Ou EPOCHS_CV\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Garantir que os scripts não tentem carregar de caminhos fixos se os overrides são dados\u001b[39;00m\n\u001b[0;32m    122\u001b[0m args_for_this_fold[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Para que os scripts não tentem construir a partir de \"data/\" global\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'EPOCHS'"
     ]
    }
   ],
   "source": [
    "# Célula 4: Loop Principal de Validação Cruzada\n",
    "\n",
    "all_machines_final_results = {}\n",
    "# Diretório para dados temporários de cada fold (será recriado a cada execução da célula)\n",
    "# Isso garante que não haja contaminação entre execuções do notebook.\n",
    "if os.path.exists(FOLD_SPECIFIC_TEMP_PATH):\n",
    "    shutil.rmtree(FOLD_SPECIFIC_TEMP_PATH)\n",
    "os.makedirs(FOLD_SPECIFIC_TEMP_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "for machine_type, machine_ids in MACHINE_CONFIGS.items():\n",
    "    for machine_id in machine_ids:\n",
    "        machine_name_id_str = f\"{machine_type}_{machine_id}\"\n",
    "        print(f\"\\n===== Processando Máquina: {machine_name_id_str} =====\")\n",
    "\n",
    "        # 1. Carregar TODOS os dados MFCC (brutos, sem scaling ainda) para esta máquina/ID\n",
    "        #    A função também salva/carrega de um cache global para evitar reprocessar .wavs\n",
    "        X_all_tensors_raw, y_all_original_labels, detected_n_coeffs = get_all_mfcc_data_for_machine(\n",
    "            machine_type, machine_id,\n",
    "            num_mfcc_coeffs=DEFAULT_ARGS_DICT[\"NUMBER_FEATURES\"], # Passa o N_MFCC desejado\n",
    "            force_reprocess=False # Mude para True se quiser forçar o reprocessamento dos .wavs\n",
    "        )\n",
    "\n",
    "        if not X_all_tensors_raw:\n",
    "            print(f\"  Sem dados para {machine_name_id_str}. Pulando.\")\n",
    "            all_machines_final_results[machine_name_id_str] = {\"error\": \"No data found or processed.\"}\n",
    "            continue\n",
    "        \n",
    "        # Atualizar o número de features nos argumentos se foi detectado dinamicamente\n",
    "        current_run_args_dict = DEFAULT_ARGS_DICT.copy()\n",
    "        current_run_args_dict[\"NUMBER_FEATURES\"] = detected_n_coeffs\n",
    "        current_run_args_dict[\"machine_type\"] = machine_type\n",
    "        current_run_args_dict[\"machine_id\"] = machine_id\n",
    "        # Garantir que FEATS seja compatível com NUMBER_FEATURES para os scripts\n",
    "        # Se NUMBER_FEATURES é, por exemplo, 20, FEATS pode ser \"all\" se for o default,\n",
    "        # ou você pode ter uma lógica para mapear NUMBER_FEATURES para uma string FEATS válida.\n",
    "        # Por simplicidade, se NUMBER_FEATURES é a fonte da verdade, FEATS é mais para nomeação.\n",
    "        # Vamos assumir que os scripts usarão args.NUMBER_FEATURES diretamente se args.FEATS não levar a um mapeamento.\n",
    "\n",
    "        # Rótulos para métricas: 1 para ANOMALIA, 0 para NORMAL\n",
    "        # Seus rótulos originais: Normal=1, Anormal=0\n",
    "        y_all_labels_anomaly_is_1 = 1 - y_all_original_labels\n",
    "\n",
    "        if len(np.unique(y_all_labels_anomaly_is_1)) < 2:\n",
    "            print(f\"  Apenas uma classe presente para {machine_name_id_str} ({np.unique(y_all_original_labels)}). CV não é possível. Pulando.\")\n",
    "            all_machines_final_results[machine_name_id_str] = {\"error\": \"Single class present.\"}\n",
    "            continue\n",
    "\n",
    "        fold_metrics_accumulator = { \"roc_auc\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": [] }\n",
    "        \n",
    "        # Diretório temporário específico para esta máquina e seus folds\n",
    "        machine_fold_temp_dir = os.path.join(FOLD_SPECIFIC_TEMP_PATH, machine_name_id_str)\n",
    "        os.makedirs(machine_fold_temp_dir, exist_ok=True)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "        # Usar np.arange pois X_all_tensors_raw é uma lista de tensores\n",
    "        indices_for_kf = np.arange(len(X_all_tensors_raw))\n",
    "\n",
    "        for fold_num, (train_indices, test_indices) in enumerate(tqdm(list(skf.split(indices_for_kf, y_all_labels_anomaly_is_1)), desc=f\" Folds {machine_name_id_str}\")):\n",
    "            print(f\"\\n  --- Fold {fold_num + 1}/{N_FOLDS} para {machine_name_id_str} ---\")\n",
    "            \n",
    "            fold_temp_dir_current = os.path.join(machine_fold_temp_dir, f\"fold_{fold_num}\")\n",
    "            os.makedirs(fold_temp_dir_current, exist_ok=True)\n",
    "\n",
    "            # Separar dados RAW do fold\n",
    "            X_train_fold_raw = [X_all_tensors_raw[i] for i in train_indices]\n",
    "            y_train_fold_original = y_all_original_labels[train_indices] # Normal=1\n",
    "            \n",
    "            X_test_fold_raw = [X_all_tensors_raw[i] for i in test_indices]\n",
    "            y_test_fold_anomaly_is_1 = y_all_labels_anomaly_is_1[test_indices] # Anomalia=1\n",
    "\n",
    "            # Isolar dados NORMAIS de treino do fold (RAW)\n",
    "            X_train_normal_fold_raw = [\n",
    "                X_train_fold_raw[i] for i, label in enumerate(y_train_fold_original) if label == 1\n",
    "            ]\n",
    "\n",
    "            if not X_train_normal_fold_raw:\n",
    "                print(f\"    Fold {fold_num + 1}: Sem dados normais para treino. Registrando NaNs.\")\n",
    "                for metric_list in fold_metrics_accumulator.values(): metric_list.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            # Normalização DENTRO DO FOLD (Opção B)\n",
    "            # Concatenar todos os frames dos dados normais de treino do fold para FITAR o scaler\n",
    "            all_train_normal_frames_list = []\n",
    "            for raw_tensor in X_train_normal_fold_raw: # raw_tensor é (1, seq_len, n_coeffs)\n",
    "                all_train_normal_frames_list.append(raw_tensor.squeeze(0).cpu().numpy())\n",
    "            \n",
    "            combined_train_normal_frames = np.concatenate(all_train_normal_frames_list, axis=0) # (total_frames_normais_fold, n_coeffs)\n",
    "            \n",
    "            scaler_fold = MinMaxScaler()\n",
    "            scaler_fold.fit(combined_train_normal_frames)\n",
    "\n",
    "            # Aplicar scaler aos dados de treino normal do fold\n",
    "            X_train_normal_fold_scaled_tensors = []\n",
    "            for raw_tensor in X_train_normal_fold_raw:\n",
    "                data_to_scale = raw_tensor.squeeze(0).cpu().numpy()\n",
    "                scaled_data = scaler_fold.transform(data_to_scale)\n",
    "                X_train_normal_fold_scaled_tensors.append(torch.tensor(scaled_data, dtype=torch.float32).unsqueeze(0).to(DEVICE))\n",
    "\n",
    "            # Aplicar scaler aos dados de teste do fold\n",
    "            X_test_fold_scaled_tensors = []\n",
    "            for raw_tensor in X_test_fold_raw:\n",
    "                data_to_scale = raw_tensor.squeeze(0).cpu().numpy()\n",
    "                scaled_data = scaler_fold.transform(data_to_scale)\n",
    "                X_test_fold_scaled_tensors.append(torch.tensor(scaled_data, dtype=torch.float32).unsqueeze(0).to(DEVICE))\n",
    "            \n",
    "            # Salvar dados SCALED do fold para o script usar\n",
    "            fold_train_data_path_script = os.path.join(fold_temp_dir_current, \"train_norm_scaled.pkl\")\n",
    "            save_fold_data_for_script(X_train_normal_fold_scaled_tensors, fold_train_data_path_script)\n",
    "            \n",
    "            fold_test_data_path_script = os.path.join(fold_temp_dir_current, \"test_scaled.pkl\")\n",
    "            save_fold_data_for_script(X_test_fold_scaled_tensors, fold_test_data_path_script)\n",
    "\n",
    "            # Configurar args para o script deste fold\n",
    "            args_for_this_fold = current_run_args_dict.copy()\n",
    "            args_for_this_fold[\"input_train_data_path\"] = fold_train_data_path_script\n",
    "            args_for_this_fold[\"input_test_data_path\"] = fold_test_data_path_script\n",
    "            args_for_this_fold[\"output_model_base_path\"] = os.path.join(fold_temp_dir_current, \"model_fold\")\n",
    "            args_for_this_fold[\"output_results_pkl_path\"] = os.path.join(fold_temp_dir_current, \"scores_fold.pkl\")\n",
    "            args_for_this_fold[\"epochs\"] = DEFAULT_ARGS_DICT[\"epochs\"] # Ou EPOCHS_CV\n",
    "            # Garantir que os scripts não tentem carregar de caminhos fixos se os overrides são dados\n",
    "            args_for_this_fold[\"data_folder\"] = \"./\" # Para que os scripts não tentem construir a partir de \"data/\" global\n",
    "\n",
    "            # Executar o script de treino/predição (WAE-GAN)\n",
    "            script_to_run = os.path.join(SCRIPTS_WAE_PATH, \"train_cycles_adversarial.py\")\n",
    "            success = run_script_for_fold(script_to_run, args_for_this_fold)\n",
    "\n",
    "            if not success:\n",
    "                print(f\"    Falha ao executar script para o fold {fold_num + 1}. Registrando NaNs.\")\n",
    "                for metric_list in fold_metrics_accumulator.values(): metric_list.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            # Carregar scores de anomalia salvos pelo script\n",
    "            recon_errors, critic_sc = load_scores_from_script_output(args_for_this_fold[\"output_results_pkl_path\"])\n",
    "\n",
    "            if recon_errors is None or len(recon_errors) != len(y_test_fold_anomaly_is_1):\n",
    "                print(f\"    Scores de anomalia não carregados ou com tamanho incorreto para o fold {fold_num + 1}. Registrando NaNs.\")\n",
    "                for metric_list in fold_metrics_accumulator.values(): metric_list.append(np.nan)\n",
    "                continue\n",
    "            \n",
    "            # Usar apenas erro de reconstrução como score de anomalia por enquanto\n",
    "            # TODO: Incorporar critic_scores se a estratégia de combinação for definida\n",
    "            anomaly_scores_fold = recon_errors\n",
    "\n",
    "            # Calcular Métricas\n",
    "            if len(np.unique(y_test_fold_anomaly_is_1)) < 2: # Checagem de segurança\n",
    "                roc_auc, acc, prec, rec, f1 = 0.5 if np.all(y_test_fold_anomaly_is_1 == y_test_fold_anomaly_is_1[0]) else np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "            else:\n",
    "                roc_auc = roc_auc_score(y_test_fold_anomaly_is_1, anomaly_scores_fold)\n",
    "                optimal_thresh = find_optimal_threshold_roc(y_test_fold_anomaly_is_1, anomaly_scores_fold)\n",
    "                y_pred_binary = (anomaly_scores_fold >= optimal_thresh).astype(int)\n",
    "                acc = accuracy_score(y_test_fold_anomaly_is_1, y_pred_binary)\n",
    "                prec = precision_score(y_test_fold_anomaly_is_1, y_pred_binary, zero_division=0)\n",
    "                rec = recall_score(y_test_fold_anomaly_is_1, y_pred_binary, zero_division=0)\n",
    "                f1 = f1_score(y_test_fold_anomaly_is_1, y_pred_binary, zero_division=0)\n",
    "\n",
    "            fold_metrics_accumulator[\"roc_auc\"].append(roc_auc)\n",
    "            fold_metrics_accumulator[\"accuracy\"].append(acc)\n",
    "            fold_metrics_accumulator[\"precision\"].append(prec)\n",
    "            fold_metrics_accumulator[\"recall\"].append(rec)\n",
    "            fold_metrics_accumulator[\"f1\"].append(f1)\n",
    "            print(f\"    Fold {fold_num + 1} Métricas: ROC-AUC={roc_auc:.4f}, F1={f1:.4f}, Acc={acc:.4f}\")\n",
    "\n",
    "        # Fim dos folds para a máquina/ID atual\n",
    "        machine_summary_stats = {}\n",
    "        valid_roc_aucs = [auc for auc in fold_metrics_accumulator[\"roc_auc\"] if not np.isnan(auc)]\n",
    "        if len(valid_roc_aucs) >= 2:\n",
    "            mean_roc_auc = np.mean(valid_roc_aucs)\n",
    "            bootstrap_means = [np.mean(resample(valid_roc_aucs, replace=True, n_samples=len(valid_roc_aucs), random_state=i)) for i in range(N_BOOTSTRAP_SAMPLES)]\n",
    "            ci_lower = np.percentile(bootstrap_means, 2.5)\n",
    "            ci_upper = np.percentile(bootstrap_means, 97.5)\n",
    "            machine_summary_stats[\"roc_auc_mean\"] = mean_roc_auc\n",
    "            machine_summary_stats[\"roc_auc_ci\"] = (ci_lower, ci_upper)\n",
    "        else:\n",
    "            machine_summary_stats[\"roc_auc_mean\"] = np.nanmean(valid_roc_aucs) if valid_roc_aucs else np.nan\n",
    "            machine_summary_stats[\"roc_auc_ci\"] = (np.nan, np.nan)\n",
    "\n",
    "        for metric_key in [\"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
    "            valid_vals = [val for val in fold_metrics_accumulator[metric_key] if not np.isnan(val)]\n",
    "            machine_summary_stats[f\"{metric_key}_mean\"] = np.mean(valid_vals) if valid_vals else np.nan\n",
    "            machine_summary_stats[f\"{metric_key}_std\"] = np.std(valid_vals) if valid_vals else np.nan\n",
    "        \n",
    "        all_machines_final_results[machine_name_id_str] = machine_summary_stats\n",
    "        print(f\"  Resultados Consolidados para {machine_name_id_str}:\")\n",
    "        print(f\"    ROC-AUC: {machine_summary_stats['roc_auc_mean']:.4f} (CI: [{machine_summary_stats['roc_auc_ci'][0]:.4f}-{machine_summary_stats['roc_auc_ci'][1]:.4f}])\")\n",
    "        print(f\"    F1-Score: {machine_summary_stats.get('f1_mean', np.nan):.4f} +/- {machine_summary_stats.get('f1_std', np.nan):.4f}\")\n",
    "\n",
    "\n",
    "# Salvar todos os resultados finais\n",
    "final_pkl_path = os.path.join(FINAL_RESULTS_PATH, \"all_machines_evaluation_summary.pkl\")\n",
    "with open(final_pkl_path, \"wb\") as f:\n",
    "    pkl.dump(all_machines_final_results, f)\n",
    "print(f\"\\nResultados finais de todas as máquinas salvos em: {final_pkl_path}\")\n",
    "\n",
    "# Limpar a pasta temporária principal após a conclusão de todas as máquinas\n",
    "# shutil.rmtree(FOLD_SPECIFIC_TEMP_PATH)\n",
    "# print(f\"Pasta temporária {FOLD_SPECIFIC_TEMP_PATH} removida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5: Geração das Tabelas LaTeX\n",
    "# (Reutilizar a função generate_latex_tables(all_machines_final_results) da resposta anterior)\n",
    "# Ex:\n",
    "# generate_latex_tables(all_machines_final_results)\n",
    "# (Você pode carregar do .pkl se executar esta célula separadamente)\n",
    "# with open(os.path.join(FINAL_RESULTS_PATH, \"all_machines_evaluation_summary.pkl\"), \"rb\") as f:\n",
    "#    loaded_results = pkl.load(f)\n",
    "# generate_latex_tables(loaded_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
